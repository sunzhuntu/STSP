{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_transition(cat_reIndex_mapping,count_dict,poi_cat_distrib,first_mall_index):    \n",
    "    category_transition= np.zeros([len(cat_reIndex_mapping), len(cat_reIndex_mapping)])\n",
    "    sum_dict=sum(count_dict.values())\n",
    "    for (m,n) in count_dict.keys():\n",
    "        #print(m)\n",
    "        if (m >(first_mall_index-1)) & (n<first_mall_index):\n",
    "            for cate_id in poi_cat_distrib[m].keys():\n",
    "                total_num=sum(poi_cat_distrib[m].values())\n",
    "                ratio=poi_cat_distrib[m][cate_id] / total_num \n",
    "                category_transition[cate_id][n]+=ratio*count_dict[m,n]\n",
    "        else:\n",
    "            if (m <first_mall_index)& (n>(first_mall_index-1)):\n",
    "                for cate_id in poi_cat_distrib[n].keys():\n",
    "                    total_num=sum(poi_cat_distrib[n].values())\n",
    "                    ratio=poi_cat_distrib[n][cate_id] / total_num \n",
    "                    category_transition[m][cate_id]+=ratio*count_dict[m,n]\n",
    "            else:\n",
    "                if (m>(first_mall_index-1)) & (n>(first_mall_index-1)):\n",
    "                    total_num1=sum(poi_cat_distrib[m].values())\n",
    "                    total_num2=sum(poi_cat_distrib[n].values())\n",
    "                    for cate_id1 in poi_cat_distrib[m].keys():\n",
    "                        ratio1=poi_cat_distrib[m][cate_id1] / total_num1\n",
    "                        for cate_id2 in poi_cat_distrib[n].keys():\n",
    "                            ratio2=poi_cat_distrib[n][cate_id2]/ total_num2 \n",
    "                            category_transition[cate_id1][cate_id2]+=ratio1*ratio2*count_dict[m,n]\n",
    "                else:\n",
    "                     category_transition[m][n]=count_dict[m,n]\n",
    "    category_transition=category_transition/sum_dict\n",
    "    return category_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = tf.placeholder(tf.float32, shape = [None, None, seq_size]) \n",
    "x_time = tf.placeholder(tf.int32, shape = [None, None]) \n",
    "x_weekend = tf.placeholder(tf.int32, shape = [None, None]) \n",
    "x_type = tf.placeholder(tf.int32, shape = [None, None]) \n",
    "y_time = tf.placeholder(tf.int32, shape = [None, 1]) \n",
    "y_weekend = tf.placeholder(tf.int32, shape = [None, 1]) \n",
    "y_type = tf.placeholder(tf.int32, shape = [None, 1])\n",
    "y_seq = tf.placeholder(tf.float32, shape = [None, 1, seq_size])\n",
    "\n",
    " \n",
    "\n",
    "neg_seq = tf.placeholder(tf.float32, [None, neg_num, seq_size]) \n",
    "neg_time = tf.placeholder(tf.int32, [None, neg_num]) \n",
    "neg_weekend = tf.placeholder(tf.int32, [None, neg_num]) \n",
    "neg_type = tf.placeholder(tf.int32, [None, neg_num]) \n",
    "\n",
    "\n",
    "\n",
    "user_emb = tf.Variable(tf.random_uniform([user_size, hidden_size], -1.0, 1.0))\n",
    "seq_emb_begin = tf.Variable(tf.random_uniform([seq_size, hidden_size], -1.0, 1.0))\n",
    "seq_emb=tf.matmul(tf.cast(seq_transition,tf.float32),seq_emb_begin)+seq_emb_begin\n",
    "type_emb = tf.Variable(tf.random_uniform([2, hidden_size], -1.0, 1.0))\n",
    "time_emb = tf.Variable(tf.random_uniform([time_size, hidden_size], -1.0, 1.0))\n",
    "weekend_emb = tf.Variable(tf.random_uniform([weekend_size, hidden_size], -1.0, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "init_weight = tf.truncated_normal([hidden_size, hidden_size], stddev = 1.0/np.sqrt(hidden_size))\n",
    "W_time = tf.Variable(init_weight)\n",
    "W_weekend = tf.Variable(init_weight)\n",
    "W_seq = tf.Variable(init_weight)\n",
    "W_type = tf.Variable(init_weight)\n",
    "W_h_c = tf.Variable(init_weight)\n",
    "W_h_t= tf.Variable(tf.truncated_normal([1, hidden_size], stddev = 1.0/np.sqrt(hidden_size)))\n",
    "\n",
    "input_x_seq = tf.matmul(x_seq, tf.expand_dims(seq_emb, 0)) \n",
    "input_x_time = tf.nn.embedding_lookup(time_emb, x_time) \n",
    "input_x_weekend = tf.nn.embedding_lookup(weekend_emb, x_weekend)\n",
    "input_x_type = tf.nn.embedding_lookup(type_emb, x_type) \n",
    "\n",
    "inputs_x=tf.matmul(input_x_time,tf.expand_dims(W_time,0)) +tf.matmul(input_x_weekend, tf.expand_dims(W_weekend,0))+ tf.matmul(input_x_seq, tf.expand_dims(W_seq,0)) + tf.matmul(input_x_type, tf.expand_dims(W_type,0)) \n",
    "\n",
    "\n",
    "input_y_seq = tf.matmul(y_seq, tf.expand_dims(seq_emb, 0)) \n",
    "input_y_time = tf.nn.embedding_lookup(time_emb, y_time) \n",
    "input_y_weekend = tf.nn.embedding_lookup(weekend_emb, y_weekend)\n",
    "input_y_type = tf.nn.embedding_lookup(type_emb, y_type) \n",
    "\n",
    "inputs_y=tf.matmul(input_y_time, tf.expand_dims(W_time,0)) + tf.matmul(input_y_weekend, tf.expand_dims(W_weekend,0)) +tf.matmul(input_y_seq, tf.expand_dims(W_seq,0)) + tf.matmul(input_y_type, tf.expand_dims(W_type,0)) \n",
    "\n",
    "\n",
    "input_neg_seq = tf.matmul(neg_seq, tf.expand_dims(seq_emb, 0)) \n",
    "input_neg_time = tf.nn.embedding_lookup(time_emb, y_time) \n",
    "input_neg_weekend = tf.nn.embedding_lookup(weekend_emb, y_weekend) \n",
    "input_neg_type = tf.nn.embedding_lookup(type_emb, neg_type) \n",
    "\n",
    "inputs_neg = tf.matmul(input_neg_time, tf.expand_dims(W_time,0)) + tf.matmul(input_neg_weekend, tf.expand_dims(W_weekend,0))+ tf.matmul(input_neg_seq, tf.expand_dims(W_seq,0)) + tf.matmul(input_neg_type, tf.expand_dims(W_type,0))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.variable_scope(\"rnn\"):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    \n",
    "    cell= tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell= tf.nn.rnn_cell.MultiRNNCell([cell] * RNN_stack_layers\n",
    "                                      \n",
    "    initial_state= cell.zero_state(batch_size, tf.float32)\n",
    "                                      \n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, inputs = inputs_x, initial_state = initial_state)\n",
    "                                      \n",
    "                                      \n",
    "                                      \n",
    "regularization = tf.nn.l2_loss(W_time) + tf.nn.l2_loss(W_weekend) +tf.nn.l2_loss(W_seq) + tf.nn.l2_loss(seq_emb) + tf.nn.l2_loss(time_emb) +tf.nn.l2_loss(weekend_emb)+ tf.nn.l2_loss(W_h_c) + tf.nn.l2_loss(W_h_t) + tf.nn.l2_loss(W_type) + tf.nn.l2_loss(type_emb)   \n",
    "                                     \n",
    "                                      \n",
    "final_output= tf.expand_dims(tf.transpose(outputs, [1, 0, 2])[-1], 1)  \n",
    "                                                                         \n",
    "output_h_c = tf.matmul(final_output, tf.expand_dims(W_h_c,0))\n",
    "                                      \n",
    "seq = tf.matmul(output_h_c, tf.transpose(inputs_y, [0, 2, 1])) \n",
    "                                      \n",
    "seq_neg = tf.matmul(output_h_c, tf.transpose(inputs_neg, [0, 2, 1])) \n",
    "                                      \n",
    "loss_seq = tf.reduce_sum(1 + tf.log(tf.exp(-(tf.tile(seq, [0, 0, neg_num]) - seq_neg)))) \n",
    "\n",
    "                                      \n",
    "\n",
    "output_h_t = tf.matmul(final_output, tf.expand_dims(tf.transpose(W_h_t),0)) \n",
    "                                      \n",
    "pred_t = tf.reduce_sum(tf.sigmoid(output_h_t)) \n",
    "                                      \n",
    "loss_type = - (tf.cast(tf.reduce_sum(y_type), tf.float32) * tf.log(pred_t) + tf.cast((1-tf.reduce_sum(y_type)), tf.float32) * tf.log(1 - pred_t))\n",
    "\n",
    "#loss\n",
    "\n",
    "total_loss = w_c * loss_seq + w_t * loss_type + reg_beta * regularization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "gradients, variables = zip(*optimizer.compute_gradients(total_loss))\n",
    "\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "train = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    entropy_loss = []\n",
    "    prev_loss = 10000\n",
    "    try:    \n",
    "        for i in range(iter_num):\n",
    "            user_counter = 0\n",
    "\n",
    "            sample_counter = 0\n",
    "\n",
    "            iter_total_loss = 0.0\n",
    "\n",
    "            for user_training_samples in all_training_samples:\n",
    "\n",
    "                for sample in user_training_samples:\n",
    "                    \n",
    "                    feed_dict = data_feeder(sample)\n",
    "\n",
    "                    _, _loss = sess.run([train, total_loss], \n",
    "\n",
    "                                        {x_time: feed_dict['x_time'], \n",
    "                                         x_weekend: feed_dict['x_weekend'],\n",
    "                                         x_seq: feed_dict['x_seq'], \n",
    "                                         x_type: feed_dict['x_type'],                               \n",
    "                                         y_time: feed_dict['y_time'], \n",
    "                                         y_weekend: feed_dict['y_weekend'], \n",
    "                                         y_seq: feed_dict['y_seq'], \n",
    "                                         y_type: feed_dict['y_type'],\n",
    "                                         neg_seq: feed_dict['neg_seq'],\n",
    "                                         neg_type: feed_dict['neg_type'] })\n",
    "                    iter_total_loss += _loss\n",
    "\n",
    "                    sample_counter += 1                  \n",
    "\n",
    "                user_counter += 1\n",
    "\n",
    "            avg_loss = iter_total_loss / sample_counter\n",
    "            entropy_loss.append(avg_loss)\n",
    "            delta=prev_loss - avg_loss\n",
    "            if delta > break_threshold: \n",
    "                prev_loss = avg_loss  \n",
    "            else: \n",
    "                raise StopIteration\n",
    "    except StopIteration:\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
