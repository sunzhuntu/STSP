{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import Util\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax #one-hot decoder  inverted = argmax(encoded[0])\n",
    "from keras.utils import to_categorical #one-hot  encoded = to_categorical(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 120)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data\n",
    "df_POI = pd.read_csv('data/CHA/1_lstm_POI.csv')\n",
    "df_checkin = pd.read_csv('data/0_CHA_Category_Pops_checkin.csv')\n",
    "test_set = pd.read_csv('data/CHA/1_lstm_Test.csv')\n",
    "train_set = pd.read_csv('data/CHA/1_lstm_Train.csv')\n",
    "\n",
    "#prepare for one-hot,reindex\n",
    "\n",
    "\n",
    "#POI_pop\n",
    "POI_pop2Index = Util.value2index(df_POI,'POI_id_Popular')\n",
    "\n",
    "#category_num\n",
    "Category_num2Index = Util.value2index(df_POI,'category_Pnum')\n",
    "\n",
    "#userId & embedding relation\n",
    "with open(\"data/CHA/1_lstm_userID_old2new.pickle\", \"rb\") as fp:   #读取\n",
    "    userID_old2new = pickle.load(fp)\n",
    "\n",
    "\n",
    "user_matrix = np.load('category result/CHA/week_user_rep_CHA/' + str(0) + '_full.npy', allow_pickle=True) \n",
    "m = (user_matrix).shape[2]\n",
    "user_matrix = (user_matrix).reshape(1,m)\n",
    "for i in range(1,len(userID_old2new.keys())):\n",
    "    user_rep_vec1 = np.load('category result/CHA/week_user_rep_CHA/' + str(i) + '_full.npy', allow_pickle=True) \n",
    "    user_rep_vec1 = (user_rep_vec1).reshape(1,m)\n",
    "    user_matrix = np.concatenate((user_matrix,user_rep_vec1),axis=0)\n",
    "user_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, entropy loss: 0.613347\n",
      "iteration: 1, entropy loss: 0.473213\n",
      "iteration: 2, entropy loss: 0.400127\n",
      "iteration: 3, entropy loss: 0.353886\n",
      "iteration: 4, entropy loss: 0.322897\n",
      "iteration: 5, entropy loss: 0.301156\n",
      "iteration: 6, entropy loss: 0.285207\n",
      "iteration: 7, entropy loss: 0.273178\n",
      "iteration: 8, entropy loss: 0.263891\n",
      "iteration: 9, entropy loss: 0.256583\n",
      "iteration: 10, entropy loss: 0.250779\n",
      "iteration: 11, entropy loss: 0.246088\n",
      "iteration: 12, entropy loss: 0.242287\n",
      "iteration: 13, entropy loss: 0.239159\n",
      "iteration: 14, entropy loss: 0.236603\n",
      "iteration: 15, entropy loss: 0.234463\n",
      "iteration: 16, entropy loss: 0.232711\n",
      "iteration: 17, entropy loss: 0.231228\n",
      "iteration: 18, entropy loss: 0.230000\n",
      "iteration: 19, entropy loss: 0.228962\n",
      "iteration: 20, entropy loss: 0.228088\n",
      "INFO:tensorflow:Restoring parameters from model/CHA/path.ckpt\n",
      "10 category, ten POI_id result:\n",
      "MAP@10 0.061095895413380084\n",
      "recall@10 0.09815950920245399\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# import data\n",
    "\n",
    "listmax = [0,1,2,3,4,5,6,7,8,9,10,\n",
    "           11,12,13,14,15,16,17,18,19,20,\n",
    "           21,22,23,24,25,26,27,28,29,30,\n",
    "           31,32,33,34,35,36,37,38,39,40,\n",
    "           41,42,43,44,45,46,47,48,49,50,\n",
    "           51,52,53,54,55,56,57,58,59,60,\n",
    "           61,62,63,64,65,66,67,68,69,70,\n",
    "           71,72,73,74,75,76,77,78,79,80,\n",
    "           81,82,83,84,85,86,87,88,89,90,\n",
    "           91,92,93,94,95,96,97,98,99,100,\n",
    "           101,102,103,104,105,106,107,108,109,110,\n",
    "           111,112,113,114,115,116,117,118,119,120,\n",
    "           121,122,123,124,125,126,127,128,129,130,\n",
    "           131,132,133,134,135,136,137,138,139,140,\n",
    "           141,142,143,144,145,146,147,148,149,150\n",
    "          ]\n",
    "\n",
    "listprefer = [0,1,2,3,4,5,6,7,8,9,10,\n",
    "           11,12,13,14,15,16,17,18,19,20,\n",
    "           21,22,23,24,25,26,27,28,29,30,\n",
    "           31,32,33,34,35,36,37,38,39,40,\n",
    "           41,42,43,44,45,46,47,48,49,50,\n",
    "           51,52,53,54,55,56,57,58,59,60,\n",
    "           61,62,63,64,65,66,67,68,69,70,\n",
    "           71,72,73,74,75,76,77,78,79,80,\n",
    "           81,82,83,84,85,86,87,88,89,90,\n",
    "           91,92,93,94,95,96,97,98,99,100]\n",
    "\n",
    "#one-hot\n",
    "\n",
    "POI_popularity_matrix = to_categorical(list(POI_pop2Index.values()))\n",
    "POI_number_matrix = to_categorical(list(Category_num2Index.values()))\n",
    "user_Prefer_matrix = to_categorical(listprefer) # max value is 100\n",
    "distance_matrix = to_categorical(listmax) # max value is 150\n",
    "m = m #user matrix\n",
    "\n",
    "\n",
    "# set parameters\n",
    "lr = 0.0001 #learning rate\n",
    "iter_num = 100 #no. of iteration\n",
    "reg_beta = 0.0025 # overfit control\n",
    "break_threshold = 0.001# iteration control\n",
    "seed = 6\n",
    "\n",
    "candidate_num = 10\n",
    "if candidate_num == 1:\n",
    "    Category_name = 'Category1'\n",
    "elif candidate_num == 5:\n",
    "    Category_name = 'Category5'\n",
    "elif candidate_num == 10:\n",
    "    Category_name = 'Category10'\n",
    "\n",
    "distance_size = distance_matrix.shape[1]\n",
    "pop_size = POI_popularity_matrix.shape[1]\n",
    "number_size = POI_number_matrix.shape[1]\n",
    "prefer_size = user_Prefer_matrix.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "# define placeholders (inputs)\n",
    "alpha = tf.placeholder(tf.float32,shape=None)\n",
    "truth = tf.placeholder(tf.float32,shape=None)\n",
    "user_embedding = tf.placeholder(tf.float32, shape=[1, m])\n",
    "distance = tf.placeholder(tf.float32, shape=[1, distance_size])\n",
    "POI_popularity = tf.placeholder(tf.float32, shape=[1, pop_size])\n",
    "POI_number = tf.placeholder(tf.float32, shape=[1, number_size])\n",
    "user_Prefer = tf.placeholder(tf.float32, shape=[1, prefer_size])\n",
    "\n",
    "\n",
    "# define variables (weights)\n",
    "#init_weight = tf.truncated_normal([n , m], stddev = 1.0/np.sqrt(n))\n",
    "d_weight = tf.Variable(tf.truncated_normal([distance_size , m], stddev = 1.0/np.sqrt(distance_size),seed = seed))\n",
    "IP_weight = tf.Variable(tf.truncated_normal([pop_size , m], stddev = 1.0/np.sqrt(pop_size),seed = seed))\n",
    "MP_weight = tf.Variable(tf.truncated_normal([pop_size , m], stddev = 1.0/np.sqrt(pop_size),seed = seed))\n",
    "num_weight = tf.Variable(tf.truncated_normal([number_size , m], stddev = 1.0/np.sqrt(number_size),seed = seed))\n",
    "perfer_weight = tf.Variable(tf.truncated_normal([prefer_size , m], stddev = 1.0/np.sqrt(prefer_size),seed = seed))\n",
    "\n",
    "\n",
    "\n",
    "# construct model   \n",
    "r = ((1 - alpha) * (tf.matmul(distance, d_weight) + tf.matmul(POI_popularity, IP_weight))\n",
    "     + alpha * (tf.matmul(distance, d_weight) + tf.matmul(POI_popularity, MP_weight) + tf.matmul(POI_number, num_weight))) + (tf.matmul(user_Prefer, perfer_weight))\n",
    "    \n",
    "r = tf.transpose(r)\n",
    "score = tf.matmul(user_embedding , r)# (1,1)\n",
    "\n",
    "regularization1 = tf.nn.l2_loss(d_weight) + tf.nn.l2_loss(IP_weight) + tf.nn.l2_loss(perfer_weight)\n",
    "regularization2 = tf.nn.l2_loss(d_weight) + tf.nn.l2_loss(MP_weight) + tf.nn.l2_loss(num_weight) + tf.nn.l2_loss(perfer_weight)\n",
    "\n",
    "#loss = tf.math.square(truth - score) #  non regularization\n",
    "loss = (1 - alpha) * (tf.math.square(truth - score) + reg_beta * regularization1) + alpha *(tf.math.square(truth - score) + reg_beta * regularization2)\n",
    "\n",
    "\n",
    "# define training algorithm\n",
    "train = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# train\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    entropy_loss = []\n",
    "    iter_result = 0.1\n",
    "    i = 0\n",
    "    #'Distance', 'Location_id1', 'Location_id2',\n",
    "    #'POI1_category', 'POI2_category', 'POI_Popular1', 'POI_Popular2',\n",
    "    #'POI_id1', 'POI_id2', 'Type1', 'Type2', 'User_id', 'category_Pnum1',\n",
    "    #'category_Pnum2', 'ground_True', 'user_id_reindex', 'user_preference'\n",
    "    while iter_result > break_threshold:\n",
    "        \n",
    "        if i < iter_num:\n",
    "            sample_counter = 0\n",
    "            iter_total_loss = 0.0\n",
    "            for index,row in train_set.iterrows():\n",
    "                if (row['Type2'] == 'Combined'):\n",
    "                     _, _loss = sess.run([train,loss], {alpha: 1,\n",
    "                                     truth: row['ground_True'],\n",
    "                                     distance: (distance_matrix[row['Distance']]).reshape(1,distance_size),\n",
    "                                     POI_popularity: (POI_popularity_matrix[POI_pop2Index[row['POI_Popular2']]]).reshape(1,pop_size),\n",
    "                                     POI_number: (POI_number_matrix[Category_num2Index[row['category_Pnum2']]]).reshape(1,number_size),\n",
    "                                     user_Prefer: (user_Prefer_matrix[row['user_preference']]).reshape(1,prefer_size),\n",
    "                                     user_embedding: (user_matrix[userID_old2new[row['User_id']]]).reshape(1,m)                                                   \n",
    "                                    })\n",
    "                elif (row['Type2'] == 'Independent'):\n",
    "                    _, _loss = sess.run([train,loss], {alpha: 0,\n",
    "                                     truth: row['ground_True'],\n",
    "                                     distance: (distance_matrix[row['Distance']]).reshape(1,distance_size),\n",
    "                                     POI_popularity: (POI_popularity_matrix[POI_pop2Index[row['POI_Popular2']]]).reshape(1,pop_size),\n",
    "                                     POI_number: (POI_number_matrix[Category_num2Index[row['category_Pnum2']]]).reshape(1,number_size),\n",
    "                                     user_Prefer: (user_Prefer_matrix[row['user_preference']]).reshape(1,prefer_size),\n",
    "                                     user_embedding: (user_matrix[userID_old2new[row['User_id']]]).reshape(1,m) \n",
    "                                    })\n",
    "                iter_total_loss += _loss\n",
    "                sample_counter += 1\n",
    "\n",
    "            avg_loss = iter_total_loss / sample_counter\n",
    "            entropy_loss.append(avg_loss)\n",
    "            print('iteration: %d, entropy loss: %f' %(i, avg_loss))\n",
    "            \n",
    "            if i >= 1:\n",
    "                iter_result = entropy_loss[-2] - entropy_loss[-1]            \n",
    "        i+=1\n",
    "            \n",
    "    saver.save(sess, 'model/CHA/path.ckpt')\n",
    "        \n",
    "# test\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, 'model/CHA/path.ckpt')\n",
    "\n",
    "    count10 = 0\n",
    "    sample = 0\n",
    "    map10_list = []\n",
    "\n",
    "    #['User_id', 'POI_id1', 'POI_id2', 'Location_id1','Location_id2', 'Pred_Type', 'Category1', 'Category5', 'Category10']\n",
    "    for index,row in test_set.iterrows():\n",
    "        User_id = row['User_id']\n",
    "        POI_id1 = row['POI_id1']\n",
    "        POI_id2 = row['POI_id2']\n",
    "        Location_id2 = row['Location_id2']\n",
    "        Type = row['Pred_Type']\n",
    "        Category = row[Category_name]\n",
    "        df_POI_id1 = df_POI[df_POI['POI_id'] == POI_id1]\n",
    "        df_POI_id1.reset_index(drop = True,inplace = True)\n",
    "        lng1 = df_POI_id1['POI_id_Longitude'][0]\n",
    "        lat1 = df_POI_id1['POI_id_Latitude'][0]\n",
    "        \n",
    "        Type_require = df_POI['POI_Type'].map(lambda T : T == Type)\n",
    "        Catgory_require = df_POI['Category_2'].map(lambda c2 : c2 in Category)\n",
    "        df_candidate = df_POI[Type_require & Catgory_require] #10个candidate category的情况\n",
    "        df_candidate = df_candidate.drop_duplicates(subset='POI_id', keep=\"first\")\n",
    "        \n",
    "        df_result = DataFrame(columns = ['candidate_id','score'])\n",
    "        \n",
    "        for index1,row1 in df_candidate.iterrows():\n",
    "            candidate_id = row1['POI_id']\n",
    "            lat2 = row1['POI_id_Latitude']\n",
    "            lng2 = row1['POI_id_Longitude']\n",
    "            Dis = Util.haversine(lng1, lat1, lng2, lat2)\n",
    "            Distance_candidate = int(Dis)\n",
    "            \n",
    "            num = len(df_checkin[(df_checkin['User_id'] == User_id)&(df_checkin['POI_id'] == candidate_id)])\n",
    "            total = len(df_checkin[df_checkin['User_id'] == User_id])\n",
    "            preference_candidate = int((num/total)*100)    \n",
    "            \n",
    "            if (Type == 'Combined'):\n",
    "                pred = sess.run(score, {alpha: 1,\n",
    "                                     distance: (distance_matrix[Distance_candidate]).reshape(1,distance_size),\n",
    "                                     POI_popularity: (POI_popularity_matrix[POI_pop2Index[int(row1['POI_id_Popular'])]]).reshape(1,pop_size),\n",
    "                                     POI_number: (POI_number_matrix[Category_num2Index[row1['category_Pnum']]]).reshape(1,number_size),\n",
    "                                     user_Prefer: (user_Prefer_matrix[preference_candidate]).reshape(1,prefer_size),\n",
    "                                     user_embedding: (user_matrix[userID_old2new[User_id]]).reshape(1,m) \n",
    "                                    })\n",
    "            elif (Type == 'Independent'):\n",
    "                pred = sess.run(score, {alpha: 0,\n",
    "                                     distance: (distance_matrix[Distance_candidate]).reshape(1,distance_size),\n",
    "                                     POI_popularity: (POI_popularity_matrix[POI_pop2Index[int(row1['POI_id_Popular'])]]).reshape(1,pop_size),\n",
    "                                     POI_number: (POI_number_matrix[Category_num2Index[row1['category_Pnum']]]).reshape(1,number_size),\n",
    "                                     user_Prefer: (user_Prefer_matrix[preference_candidate]).reshape(1,prefer_size),\n",
    "                                     user_embedding: (user_matrix[userID_old2new[User_id]]).reshape(1,m)\n",
    "                                    })\n",
    "\n",
    "            df_result = df_result.append(DataFrame({'candidate_id':[candidate_id],'score':[pred]}))\n",
    "        df_result = df_result.sort_values(by = 'score',ascending = False)\n",
    "#-------------------------------prediction----------------------------------\n",
    "        if (Type == 'Combined'):\n",
    "            Top10 = list(df_result['candidate_id'][:10])\n",
    "            \n",
    "            POI_id_require = df_POI['POI_id'].map(lambda x : x in Top10)\n",
    "            Catgory_require = df_POI['Category_2'].map(lambda c1 : c1 in Category)\n",
    "            df_candidate_location = df_POI[POI_id_require & Catgory_require]\n",
    "            df_candidate_location = df_candidate_location.sort_values(by = 'stars',ascending = False)\n",
    "            \n",
    "            location_Top10 = list(df_candidate_location['Location_id'][:10])\n",
    "            if Location_id2 in location_Top10:\n",
    "                map10 =  (1/(location_Top10.index(Location_id2)+1))\n",
    "            else:\n",
    "                map10 = 0\n",
    "            \n",
    "            map10_list.append(map10)\n",
    "            \n",
    "            if Location_id2 in location_Top10:\n",
    "                count10 += 1\n",
    "                \n",
    "            sample += 1\n",
    "            #print(Location_id2,Type,Category,location_Top10)\n",
    "            \n",
    "        elif (Type == 'Independent'):\n",
    "            \n",
    "            Top10 = list(df_result['candidate_id'][:10])\n",
    "            \n",
    "            if POI_id2 in Top10:\n",
    "                map10 =  (1/(Top10.index(POI_id2)+1))\n",
    "            else:\n",
    "                map10 = 0\n",
    "            \n",
    "            map10_list.append(map10)\n",
    "\n",
    "           \n",
    "            if POI_id2 in Top10:\n",
    "                count10 += 1\n",
    "                \n",
    "            sample += 1\n",
    "            #print(POI_id2,Type,Category,Top10) \n",
    "        \n",
    "    MAP10 = np.mean(map10_list)\n",
    "    recall10 = count10/sample\n",
    "\n",
    "    print(str(candidate_num)+' category, ten POI_id result:')\n",
    "    print('MAP@10',MAP10)\n",
    "    print('recall@10',recall10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
